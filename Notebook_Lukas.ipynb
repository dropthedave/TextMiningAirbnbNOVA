{"cells":[{"cell_type":"markdown","metadata":{"id":"Y-M1169-7Fkt"},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16789,"status":"ok","timestamp":1685454463744,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"06dOl_gNEJQb","outputId":"90e31cba-5822-4a4d-d000-bb16b15b3e48"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package snowball_data to\n","[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package snowball_data is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["# basics\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# preprocessing\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize \n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import SnowballStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('snowball_data')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# sklearn & imblearn\n","from sklearn.model_selection import train_test_split, learning_curve\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, RocCurveDisplay\n","from sklearn.utils import compute_class_weight\n","from imblearn.over_sampling import SMOTE\n","\n","# tensorflow\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# sentiment analysis (vaderSentiment)\n","try:\n","    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","except:\n","    !pip install vaderSentiment\n","    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","# unidecode\n","try:\n","  from unidecode import unidecode\n","except:\n","  !pip install unidecode\n","  from unidecode import unidecode\n","\n","# pycld2\n","# try:\n","#   import pycld2 as cld2\n","# except:\n","#   !pip install pycld2\n","#   import pycld2 as cld2\n","\n","# Word2Vec\n","try:\n","  from gensim.models import Word2Vec\n","except:\n","  !pip install gensim\n","  from gensim.models import Word2Vec\n","\n","# # deep_translator\n","# try:\n","#   from deep_translator import GoogleTranslator\n","# except:\n","#   !pip install deep-translator\n","#   from deep_translator import GoogleTranslator"]},{"cell_type":"markdown","metadata":{"id":"mk5tfM-27Fky"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kma2qam77Fky"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd '/content/drive/MyDrive/TextMiningProject'\n","\n","train = pd.read_excel('Data/train.xlsx', index_col='index')\n","train_reviews = pd.read_excel('Data/train_reviews.xlsx', index_col='index')"]},{"cell_type":"markdown","metadata":{"id":"xqFhw_b3ohpN"},"source":["# 1. Exploratory Data Analysis\n","\n","- on whole trainset (maybe train-test split has to be done before exploration)"]},{"cell_type":"markdown","metadata":{"id":"IqVfhupcqkRs"},"source":["## 1.1 train.xlsx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_DwjhApomC5"},"outputs":[],"source":["# define dataframe variable\n","dataframe = train.copy()\n","dataframe = dataframe.reset_index()\n","\n","# define target feature\n","target_feature = 'unlisted'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":641},"executionInfo":{"elapsed":62,"status":"ok","timestamp":1685372888040,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"37ML7cc8p_5w","outputId":"4dae11e7-d231-4203-8e41-5db6b04f7fbb"},"outputs":[],"source":["# display head & tail\n","dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1685372888040,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"h19JSim7qEla","outputId":"40500b3c-6f42-4552-d755-5427e3e2fc09"},"outputs":[],"source":["# data types\n","print('Data Types:', '\\n')\n","dataframe.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1685372888041,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"msLG5vulqRKE","outputId":"947796ce-ae07-4e8a-fcc7-5391d2981122"},"outputs":[],"source":["# missing values\n","print('Missing Values:', '\\n')\n","pd.concat([dataframe.isnull().sum(), dataframe.eq('').sum()], keys=['Nulls','Empty Strings'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1685372888041,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"gmN1LvMMqUuA","outputId":"72404184-4ae8-4d70-bb04-961139bae272"},"outputs":[],"source":["# duplicated rows\n","print('Duplicated Rows:', '\\n')\n","dataframe.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dBlN3rpqYx5"},"outputs":[],"source":["# descriptive statistics\n","# print('Descriptive Statistics:', '\\n')\n","# dataframe.describe(include='all').T"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1685372888041,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"yYzwQ0vSrocf","outputId":"eafa6c49-256f-440b-bd1e-eb34000e065a"},"outputs":[],"source":["# check for imbalance\n","# Count the number of instances for each target value\n","target_counts = dataframe[target_feature].value_counts()\n","\n","# Create a pie chart\n","plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%')\n","plt.axis('equal')\n","plt.title('Target Imbalance Check')\n","plt.legend(labels=['listed', 'unlisted'])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cew43zt9qr0u"},"source":["## 1.2 train_reviews.xlsx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FAknBDRgqx-6"},"outputs":[],"source":["# define dataframe variable\n","dataframe = train_reviews.copy()\n","dataframe = dataframe.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1685372888042,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"Q2dt7knGq5j5","outputId":"abdbd158-2dc3-416b-de38-49a666c55690"},"outputs":[],"source":["# display head & tail\n","dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1685372888043,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"bxAfkveHrBSn","outputId":"e7eb7df5-59c7-4b37-868d-ce3dc5c52286"},"outputs":[],"source":["# data types\n","print('Data Types:', '\\n')\n","dataframe.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1685372888043,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"a_zihtl1rB3b","outputId":"8d6c931d-c7be-4473-d4af-cb0f0d2103e0"},"outputs":[],"source":["# missing values\n","print('Missing Values:', '\\n')\n","pd.concat([dataframe.isnull().sum(), dataframe.eq('').sum()], keys=['Nulls','Empty Strings'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":591,"status":"ok","timestamp":1685372888596,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"CJ2VLWzPrb7J","outputId":"f94d92b5-0db7-48e3-d09c-11f93b784b03"},"outputs":[],"source":["# duplicated rows\n","print('Duplicated Rows:', '\\n')\n","dataframe.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtE_LddmriGV"},"outputs":[],"source":["# descriptive statistics\n","# print('Descriptive Statistics:', '\\n')\n","# dataframe.describe(include='all').T"]},{"cell_type":"markdown","metadata":{"id":"6fSUIrfC7Fkz"},"source":["# 2. Preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"_UTeUgCKDkC3"},"source":["## 2.1 Regex Patterns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvmUJ6zNDkC3"},"outputs":[],"source":["regex_patterns = {\n","    # manually identified patterns\n","    r'_x005f_x000d_':               ' ',\n","    r'_x000d_':                     ' ',\n","    r'm2':                          'squaremeter',\n","    r'sm':                          'squaremeter',\n","    r'sqm':                         'squaremeter',\n","    r'm²':                          'squaremeter',\n","    r'license[ number]+[0-9a-z]+':  ' ',\n","    r'(\\\\b\\\\w)\\\\1+\\\\b':             ' ',\n","\n","    # basic patterns\n","    # remove url\n","    r'http\\S+':                     ' ',\n","    # remove html tags\n","    r'<.*?>':                       ' ',\n","    # remove punctuation\n","    r'[^\\w\\s]':                     ' ',\n","    # remove numbers\n","    r'\\d+':                         ' ',\n","    # remove multiple whitespace\n","    r'\\s+':                         ' ',\n","    # remove newline\n","    r'\\n':                          ' '\n","}"]},{"cell_type":"markdown","metadata":{"id":"vGCR06nSDkC3"},"source":["## 2.2 Preprocessing Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lK8IC2WW7Fkz"},"outputs":[],"source":["def text_preprocessing(dataframe):\n","  '''\n","  Preprocessing Pipeline\n","  Input: dataframe\n","  Output: transformed dataframe\n","  '''\n","\n","  categorical_features = list(dataframe.select_dtypes(exclude = np.number).columns)\n","  stop_words = set(stopwords.words('english'))\n","  lemmatizer = nltk.stem.WordNetLemmatizer()\n","  stemmers = {\n","    'es': nltk.stem.SnowballStemmer('spanish'),\n","    'pt': nltk.stem.SnowballStemmer('portuguese'),\n","    'nl': nltk.stem.SnowballStemmer('dutch'),\n","    'de': nltk.stem.SnowballStemmer('german'),\n","    'it': nltk.stem.SnowballStemmer('italian'),\n","    'da': nltk.stem.SnowballStemmer('danish'),\n","    'nb': nltk.stem.SnowballStemmer('norwegian'),\n","    'fi': nltk.stem.SnowballStemmer('finnish'),\n","    'sv': nltk.stem.SnowballStemmer('swedish')\n","  }\n","\n","  # lowercase non-numeric features and convert to ASCII using unidecode\n","  for col in categorical_features:\n","    dataframe[col] = dataframe[col].apply(lambda x: unidecode(x.lower()) if type(x) == str else '')\n","    \n","  # drop duplicates\n","  dataframe = dataframe.reset_index()\n","  dataframe = dataframe.drop_duplicates()\n","\n","  # fill empty cells\n","  for col in categorical_features:\n","    dataframe[col] = dataframe[col].fillna('')\n","\n","  # re.sub regex patterns (dictionary defined in the cell above)\n","  for col in categorical_features:\n","    for key, value in regex_patterns.items():\n","      dataframe[col] = dataframe[col].apply(lambda x: re.sub(key,value,x))\n","\n","  # detect language\n","  for col in categorical_features:\n","    new_col = col + '_lang'\n","    dataframe[new_col] = dataframe[col].apply(lambda x: cld2.detect(x)[-1][0][1] if len(x) < 5000 else 'text has more than 5k characters')\n","  \n","  # tokenize and remove stopwords\n","  for col in categorical_features:\n","    dataframe[col] = dataframe[col].apply(lambda x: [word for word in word_tokenize(x) if word not in stop_words])\n","\n","  # part-of-speech tagging (english only)\n","  for col in categorical_features:\n","    lang_col = col + '_lang'\n","    dataframe[col] = dataframe.apply(lambda row: nltk.pos_tag(row[col]) if row[lang_col] == 'en' else row[col], axis=1)\n","    # wordnet mapping\n","    pos_tag_map = {\n","    'J': wordnet.ADJ,\n","    'V': wordnet.VERB,\n","    'N': wordnet.NOUN,\n","    'R': wordnet.ADV\n","    }\n","\n","    dataframe[col] = dataframe.apply(lambda row: [(word, pos_tag_map.get(tag[0])) for word, tag in row[col]] if row[lang_col] == 'en' else row[col], axis=1)\n","  \n","  # lemmatizing (english only)\n","  for col in categorical_features:\n","    lang_col = col + '_lang'\n","    dataframe[col] = dataframe.apply(lambda row: [lemmatizer.lemmatize(word, tag) if tag else word for word, tag in row[col]] if row[lang_col] == 'en' else row[col], axis=1)\n","  \n","  # stemming (non-english contained in the stemmers dictionary)\n","  for col in categorical_features:\n","    lang_col = col + '_lang'\n","    dataframe[col] = dataframe.apply(lambda row: [stemmers.get(row[lang_col]).stem(word) if row[lang_col] in stemmers.keys() else word for word in row[col]] if row[lang_col] != 'en' else row[col], axis=1)\n","\n","  # combine again\n","  for col in categorical_features:\n","    dataframe[col] = dataframe[col].apply(lambda x: ' '.join(x))\n","\n","  return dataframe"]},{"cell_type":"markdown","metadata":{"id":"0Ltd5aUbDkC4"},"source":["## 2.3 Apply Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4_QuJANtwb5"},"outputs":[],"source":["train_preprocessed = train.copy()\n","train_preprocessed = text_preprocessing(train_preprocessed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gkht-0VLu0c0"},"outputs":[],"source":["train_reviews_preprocessed = train_reviews.copy()\n","train_reviews_preprocessed = text_preprocessing(train_reviews_preprocessed)"]},{"cell_type":"markdown","metadata":{"id":"Bt9h3ODfDkC4"},"source":["# 3. Feature Engineering (Sentiment Analysis on Comments)\n","\n","- so far only sentiment analysis for englisch comments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHpdH-SFDkC4"},"outputs":[],"source":["def sentiment_analysis(dataframe, column='comments', language_column='comments_lang', compound=False):\n","    '''\n","    Sentiment analysis using vaderSentiment\n","    Input: dataframe\n","    Output: dataframe with sentiment column\n","    '''\n","    new_column = column + '_sentiment'\n","    sia = SentimentIntensityAnalyzer()\n","\n","    # return sentiment label with highest score\n","    if compound is False:\n","      dataframe[new_column] = dataframe[column].apply(lambda x: sia.polarity_scores(x))\n","      dataframe[new_column] = dataframe[new_column].apply(lambda x: {key:val for key,val in x.items() if key != 'compound'})\n","      dataframe[new_column] = dataframe[new_column].apply(lambda x: max(x, key=x.get))\n","      # set empty string comments from 'neg' (negative) to 'neu' (neutral)\n","      dataframe[new_column] = ['neu' if y == '' else x for x,y in zip(dataframe[new_column], dataframe[column])]\n","      # exception for comments which are not in english\n","      dataframe[new_column] = ['neu' if y != 'en' else x for x,y in zip(dataframe[new_column], dataframe[language_column])]\n","\n","    # compound score is a combined score that ranges from -1 to 1\n","    # higher values indicating more positive sentiment\n","    elif compound is True:\n","      dataframe[new_column] = dataframe[column].apply(lambda x: sia.polarity_scores(x)['compound'])\n","      # exception for comments which are not in english\n","      dataframe[new_column] = [0 if y != 'en' else x for x,y in zip(dataframe[new_column], dataframe[language_column])]\n","\n","    return dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otOcRcemDkC4"},"outputs":[],"source":["train_reviews_sentiment = train_reviews_preprocessed.copy()\n","train_reviews_sentiment = sentiment_analysis(train_reviews_sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQXXWGAyDkC5"},"outputs":[],"source":["train_reviews_sentiment_compound = train_reviews_preprocessed.copy()\n","train_reviews_sentiment_compound = sentiment_analysis(train_reviews_sentiment_compound, compound=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uz-1qjnaDkC5"},"outputs":[],"source":["# train_preprocessed.to_csv('Data/train_preprocessed.csv', index=False)\n","# train_reviews_sentiment.to_csv('Data/train_reviews_sentiment.csv', index=False)\n","# train_reviews_sentiment_compound.to_csv('Data/train_reviews_sentiment_compound.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"6tyRQu25EJQi"},"source":["# 4. Train-Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13287,"status":"ok","timestamp":1685296538568,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"npfV6P3ADkC5","outputId":"ac5aae8c-8b86-4ed4-ea81-2cb0005c13f8"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# %cd '/content/drive/MyDrive/TextMiningProject'\n","\n","# # read in preprocessed data (so preprocessing doesn't have to be done again)\n","# # keep_default_na=False to prevent empty strings from being read in as NaN\n","\n","# train_preprocessed = pd.read_csv('Data/train_preprocessed.csv', keep_default_na=False)\n","# train_reviews_sentiment = pd.read_csv('Data/train_reviews_sentiment.csv', keep_default_na=False)\n","# train_reviews_sentiment_compound = pd.read_csv('Data/train_reviews_sentiment_compound.csv', keep_default_na=False)"]},{"cell_type":"markdown","metadata":{"id":"eP2acGODEJQj"},"source":["## 4.1 Combine Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3tgPHaJDkC6"},"outputs":[],"source":["# combine all text for an airbnb and create one BoW per airbnb\n","\n","def combine_text(dataframe1=train_preprocessed, dataframe2=train_reviews_sentiment):\n","    '''\n","    Combine all text for an airbnb\n","    Input: dataframe1 (train_preprocessed), dataframe2 (train_reviews_sentiment)\n","    Output: combined dataframe\n","    Output format: 'unlisted', 'text'\n","    '''\n","    # prepare dataframe1\n","    # combine description and host_about\n","    dataframe1['text'] = [x + ' ' + y for x,y in zip(dataframe1['description'], dataframe1['host_about'])]\n","    # drop description, host_about, description_lang, host_about_lang\n","    dataframe1 = dataframe1.drop(['description', 'host_about', 'description_lang', 'host_about_lang'], axis=1)\n","\n","    # prepare dataframe2\n","    # combine all text comments for an index\n","    aggregated_comments = pd.DataFrame(dataframe2.groupby('index')['comments'].agg(lambda x: ' '.join(x))).reset_index()\n","    # combine all sentiment labels for an index\n","    aggregated_sentiment_label = pd.DataFrame(dataframe2.groupby('index')['comments_sentiment'].agg(lambda x: ' '.join(x))).reset_index()\n","    # add sentiment_labels to aggregated_comments\n","    aggregated_comments['comments'] = [x + ' ' + y for x,y in zip(aggregated_comments['comments'], aggregated_sentiment_label['comments_sentiment'])]\n","    dataframe2 = aggregated_comments\n","\n","    # merge dataframe1 and dataframe2 on index\n","    combined = pd.merge(dataframe1, dataframe2, on='index', how='left')\n","    # fill empty cells for missing comments\n","    combined = combined.fillna('')\n","    # combine all text for an airbnb\n","    combined['text'] = [x + ' ' + y for x,y in zip(combined['text'], combined['comments'])]\n","    # drop comments\n","    combined = combined.drop(['comments'], axis=1)\n","    # set index to 'index' or airbnb id\n","    combined = combined.set_index('index')\n","\n","    return combined"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"elapsed":2811,"status":"ok","timestamp":1685296541361,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"_E818W8tEJQj","outputId":"6fa65447-e6fa-46df-e576-bfcbe670a2ac"},"outputs":[],"source":["train_preprocessed_to_combine, train_reviews_sentiment_to_combine = train_preprocessed.copy(), train_reviews_sentiment.copy()\n","train_combined = combine_text(train_preprocessed_to_combine, train_reviews_sentiment_to_combine)\n","train_combined.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uOev5I-gwJe"},"outputs":[],"source":["# train_combined.to_csv('Data/train_combined.csv')"]},{"cell_type":"markdown","metadata":{"id":"UgW3LtGkEJQj"},"source":["## 4.2 Train-Test Split"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7948,"status":"ok","timestamp":1685454472531,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"FCp04qY6PXkv","outputId":"b0e415f0-adc9-41a8-84d0-098199eadd31"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# %cd '/content/drive/MyDrive/TextMiningProject'\n","\n","train_combined = pd.read_csv('Data/train_combined.csv', keep_default_na=False)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1685454472532,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"zIiGR3wnEJQj"},"outputs":[],"source":["x, y = train_combined['text'], train_combined['unlisted']\n","xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, stratify=y, random_state=420)\n","xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.2, stratify=ytrain, random_state=420)"]},{"cell_type":"markdown","metadata":{"id":"sNa7WxNRn3QF"},"source":["# 5. Encoding & SMOTE"]},{"cell_type":"markdown","metadata":{"id":"5TqO0FNxDkC6"},"source":["## 5.2 Bag-of-Words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MG2ISUYWEJQj"},"outputs":[],"source":["# def bow_encoder(xtrain=xtrain, xtest=xval, ytrain=ytrain):\n","#   '''\n","#   Encodes the text data using bag-of-words vectorizer and resamples the data using SMOTE\n","#   Input: xtrain, xtest, ytrain (needed for SMOTE)\n","#   Output: xtrain_bow_resampled, xtest_bow, ytrain_resampled\n","#   '''\n","#   # initialize bag-of-words vectorizer\n","#   bow = CountVectorizer()\n","\n","#   # transform xtrain and xval\n","#   xtrain_bow = bow.fit_transform(xtrain).toarray()\n","#   xtest_bow = bow.transform(xtest).toarray()\n","\n","#   # # SMOTE\n","#   # # initialize SMOTE\n","#   # smote = SMOTE()\n","\n","#   # # fit and resample using SMOTE\n","#   # xtrain_bow_resampled, ytrain_resampled = smote.fit_resample(xtrain_bow, ytrain)\n","\n","#   # return xtrain_bow_resampled, xtest_bow, ytrain_resampled\n","\n","#   return xtrain_bow, xtest_bow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OX1mr9fGYVO8"},"outputs":[],"source":["# xtrain_bow, xval_bow = bow_encoder(xtrain, xval)"]},{"cell_type":"markdown","metadata":{"id":"NvsLL6meDkC6"},"source":["## 5.3 TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gd5UzFciB1A9"},"outputs":[],"source":["# def tfidf_encoder(xtrain=xtrain, xtest=xval, ytrain=ytrain):\n","#   '''\n","#   Encodes the text data using tfidf vectorizer and resamples the data using SMOTE\n","#   Input: xtrain, xtest, ytrain (needed for SMOTE)\n","#   Output: xtrain_word_resampled, xtest_word, ytrain_resampled\n","#   '''\n","#   # initialize tfidf vectorizer\n","#   word_tfidf = TfidfVectorizer()\n","\n","#   # transform xtrain and xval\n","#   xtrain_word = word_tfidf.fit_transform(xtrain).toarray()\n","#   xtest_word = word_tfidf.transform(xtest).toarray()\n","\n","#   # # SMOTE\n","#   # # initialize SMOTE\n","#   # smote = SMOTE()\n","\n","#   # # fit and resample using SMOTE\n","#   # xtrain_word_resampled, ytrain_resampled = smote.fit_resample(xtrain_word, ytrain)\n","\n","#   # return xtrain_word_resampled, xtest_word, ytrain_resampled\n","\n","#   return xtrain_word, xtest_word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCvjMWaAZkwL"},"outputs":[],"source":["# xtrain_word, xval_word = tfidf_encoder(xtrain, xval)"]},{"cell_type":"markdown","metadata":{"id":"BwLFxcrQDkC6"},"source":["## 5.4 Word Embeddings\n","\n","maybe weighted avg. better instead of only np.mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JJzwxzkhhRe"},"outputs":[],"source":["# #@title Word2Vec\n","\n","# def word_to_vec(xtrain=xtrain, xtest=xval, ytrain=ytrain):\n","#   '''\n","#   Encodes the text data using Word2Vec and resamples the data using SMOTE\n","#   Input: xtrain, xtest, ytrain (needed for SMOTE)\n","#   Output: train_encoded, test_encoded, ytrain_resampled\n","#   '''\n","#   # Tokenize the texts\n","#   train_tokens = xtrain.apply(lambda x: x.split())\n","#   test_tokens = xtest.apply(lambda x: x.split())\n","  \n","#   # Train Word2Vec model\n","#   model = Word2Vec(sentences=train_tokens, window=5, min_count=1)\n","\n","#   # Encode train and validation texts\n","#   train_encoded = np.array([np.mean([model.wv[token] for token in text if token in model.wv], axis=0) for text in train_tokens])\n","#   test_encoded = np.array([np.mean([model.wv[token] for token in text if token in model.wv], axis=0) for text in test_tokens])\n","\n","#   # # SMOTE\n","#   # # initialize SMOTE\n","#   # smote = SMOTE()\n","  \n","#   # # fit and resample using SMOTE\n","#   # train_encoded_resampled, ytrain_resampled = smote.fit_resample(train_encoded, ytrain)\n","  \n","#   # return train_encoded_resampled, test_encoded, ytrain_resampled\n","\n","#   return train_encoded, test_encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FwrGCMecaec"},"outputs":[],"source":["# xtrain_embed, xval_embed = word_to_vec(xtrain, xval)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"W6mcD0RqLaPb"},"outputs":[],"source":["#@title Bert\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PbyGM63rDkC6"},"source":["# 6. Modelling"]},{"cell_type":"markdown","metadata":{"id":"QBVZXN22Hrba"},"source":["## 6.1 Modelling Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1685454472533,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"cafKjEntHoj3"},"outputs":[],"source":["def multiple_roc_auc(classifiers, xtrain, xval, ytrain, yval):\n","    '''\n","    Plots ROC/AUC curves for multiple classifiers.\n","    Input: classifiers (dict), xtrain, ytrain, xval, yval\n","    Output: ROC/AUC curves plot\n","    '''\n","\n","    fig, ax = plt.subplots(1, figsize=(15, 10))\n","    for name, clf in classifiers.items():\n","        clf.fit(xtrain, ytrain)\n","        RocCurveDisplay.from_estimator(clf, xval, yval, ax=ax, name=name)\n","    ax.set_title('Receiver Operating Characteristic (ROC)')\n","    ax.plot([0,1], [0,1], linestyle='--')\n","    return plt.show()\n","\n","def learning_curves(estimator, x, y, cv=10, scoring='f1'):\n","    '''\n","    Plots learning curve for different training set sizes\n","    Input: estimator, x, y, (optional: cv, scoring)\n","    Output: learning curve plot\n","    '''\n","\n","    train_sizes, train_scores, validation_scores = learning_curve(estimator, x, y, cv=cv, scoring=scoring, train_sizes=np.arange(.05,1,.05))\n","    train_mean, test_mean, train_std, test_std = np.mean(train_scores, axis=1), np.mean(validation_scores, axis=1), np.std(train_scores, axis=1), np.std(validation_scores, axis=1)\n","\n","    plt.subplots(1, figsize=(10,10))\n","    plt.plot(train_sizes, train_mean, color='salmon',  label='Training score', marker = 'o')\n","    plt.plot(train_sizes, test_mean, color='olive', label='Cross-validation score', marker = 's')\n","    plt.title('Learning Curve')\n","    plt.xlabel('Training Set Size')\n","    plt.ylabel(f'{scoring.upper()} Score')\n","    plt.legend(loc='best')\n","    return plt.show()\n","\n","def plot_accuracy_loss(history):\n","    '''\n","    Plots accuracy and loss curves of tensorflow model\n","    Input: fitted tf model\n","    Output: accuracy & loss plot\n","    '''\n","    # Access the accuracy and loss values from the history\n","    accuracy = history.history['accuracy']\n","    val_accuracy = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    # Plot accuracy\n","    plt.plot(accuracy, label='Training Accuracy')\n","    plt.plot(val_accuracy, label='Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.show()\n","\n","    # Plot loss\n","    plt.plot(loss, label='Training Loss')\n","    plt.plot(val_loss, label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    return plt.show()\n","\n","def class_weights_calc(ytrain):\n","\n","    # Compute class weights\n","    class_weights = compute_class_weight(\n","                                        class_weight=\"balanced\",\n","                                        classes=np.unique(ytrain.values),\n","                                        y=ytrain.values\n","                                        )\n","    class_weights_dict = dict(zip(np.unique(ytrain.values), class_weights))\n","\n","    return class_weights_dict"]},{"cell_type":"markdown","metadata":{"id":"AL1IZqSoDkC6"},"source":["## 6.2 Baseline Models"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1685454472534,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"ncxnu9JyIhyL"},"outputs":[],"source":["# # classifiers to check\n","# classifiers = {\n","#                 'K-Nearest Neighbors': KNeighborsClassifier(),\n","#                 'Logistic Regression': LogisticRegression(),\n","#                 'Random Forest': RandomForestClassifier()\n","#                 }\n","\n","# multiple_roc_auc(classifiers, xtrain_word, xval_word, ytrain, yval)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1685454472535,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"KX64d5xIEk4l"},"outputs":[],"source":["# calculate class weigths due to imbalanced dataset\n","class_weights_dict = class_weights_calc(ytrain)"]},{"cell_type":"markdown","metadata":{"id":"1AvqxlPDDkC7"},"source":["### 6.2.1 K-Nearest-Neighbor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksMGYXI92q2i"},"outputs":[],"source":["# # Bag-of-Words\n","\n","# knn = KNeighborsClassifier(n_jobs=-1)\n","# # fit model on training data\n","# # knn.fit(xtrain_bow, ytrain_bow)\n","# knn.fit(xtrain_bow, ytrain)\n","# # make predictions on validation data\n","# ypred = knn.predict(xval_bow)\n","# print(classification_report(yval, ypred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWt_bBmoDkC7"},"outputs":[],"source":["# # TF-IDF\n","\n","# knn = KNeighborsClassifier(n_jobs=-1)\n","# # fit model on training data\n","# # knn.fit(xtrain_word, ytrain_word)\n","# knn.fit(xtrain_word, ytrain)\n","# # make predictions on validation data\n","# ypred = knn.predict(xval_word)\n","# print(classification_report(yval, ypred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6orLijiM9oJ"},"outputs":[],"source":["# # Word Embeddings\n","\n","# knn = KNeighborsClassifier(n_jobs=-1)\n","# # fit model on training data\n","# # knn.fit(xtrain_embed, ytrain_embed)\n","# knn.fit(xtrain_embed, ytrain)\n","# # make predictions on validation data\n","# ypred = knn.predict(xval_embed)\n","# print(classification_report(yval, ypred))"]},{"cell_type":"markdown","metadata":{"id":"YTwe2XxkDkC7"},"source":["### 6.2.2 RandomForest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYPvkLGU2xCC"},"outputs":[],"source":["# # Bag-of-Words\n","\n","# rf = RandomForestClassifier(n_jobs=-1)\n","# # fit model on training data\n","# # rf.fit(xtrain_bow, ytrain_bow)\n","# rf.fit(xtrain_bow, ytrain)\n","# # make predictions on validation data\n","# ypred = rf.predict(xval_bow)\n","# print(classification_report(yval, ypred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTu5iVEZDkC7"},"outputs":[],"source":["# # TF-IDF\n","\n","# rf = RandomForestClassifier(n_jobs=-1)\n","# # fit model on training data\n","# # rf.fit(xtrain_word, ytrain_word)\n","# rf.fit(xtrain_word, ytrain)\n","# # make predictions on validation data\n","# ypred = rf.predict(xval_word)\n","# # print classification report\n","# print(classification_report(yval, ypred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sl8FDNGENLsN"},"outputs":[],"source":["# # Word Embedding\n","\n","# rf = RandomForestClassifier(n_jobs=-1)\n","# # fit model on training data\n","# # rf.fit(xtrain_embed, ytrain_embed)\n","# rf.fit(xtrain_embed, ytrain)\n","# # make predictions on validation data\n","# ypred = rf.predict(xval_embed)\n","# # print classification report\n","# print(classification_report(yval, ypred))"]},{"cell_type":"markdown","metadata":{"id":"6vRRowa6DkC7"},"source":["## 6.3 Neural Networks"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1685454472535,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"j5oedkMMMcmC"},"outputs":[],"source":["def tensorflow_data(xtrain=xtrain, xtest=xval, ytrain=ytrain, ytest=yval):\n","    '''\n","    Transforms the train/test split into TensorFlow datasets\n","    Input: xtrain, xtest, ytrain, ytest\n","    Output: train_dataset, test_dataset\n","    '''\n","\n","    # Convert the data to TensorFlow datasets\n","    train_dataset = tf.data.Dataset.from_tensor_slices((xtrain.values, tf.reshape(ytrain.values, (-1, 1))))\n","    test_dataset = tf.data.Dataset.from_tensor_slices((xtest.values, tf.reshape(ytest.values, (-1, 1))))\n","\n","    return train_dataset, test_dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":432,"status":"ok","timestamp":1685454472943,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"41uvKsKCN-Pg"},"outputs":[],"source":["# data for simple neural network\n","train, val = tensorflow_data(xtrain=xtrain, xtest=xval, ytrain=ytrain, ytest=yval)\n","\n","# data for RNN/LSTM\n","batch_size = 100\n","train_batch, val_batch = train.batch(batch_size).prefetch(tf.data.AUTOTUNE), val.batch(batch_size).prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685454472944,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"d1_mRD-kFljo"},"outputs":[],"source":["# define early stopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5)"]},{"cell_type":"markdown","metadata":{"id":"00TFvEUe-2qJ"},"source":["### 6.3.1 Basic Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDja-atut7LA"},"outputs":[],"source":["# # create vocabulary\n","# dataset = train\n","\n","# vocab_size = 1000\n","# encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n","# encoder.adapt(dataset.map(lambda text, label: text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDZ_IBv6DkC7"},"outputs":[],"source":["# # define the model\n","# model = tf.keras.Sequential([\n","#     # encoding layer\n","#     encoder,\n","#     # embedding layer\n","#     tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True),\n","#     # flatten layer\n","#     tf.keras.layers.Flatten(),\n","#     # dense hidden layer\n","#     tf.keras.layers.Dense(240, activation='relu'),\n","#     # dense hidden layer\n","#     tf.keras.layers.Dense(120, activation='relu'),\n","#     # output layer\n","#     tf.keras.layers.Dense(1, activation='sigmoid')\n","#     ])\n","\n","# # compile the model\n","# model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","#               optimizer=tf.keras.optimizers.Adam(),\n","#               metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","\n","# # fit the model to the training data\n","# nn = model.fit(train, validation_data=val, epochs=10, callbacks=[early_stopping], class_weight=class_weights_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdJThW-6_KSZ"},"outputs":[],"source":["# plot_accuracy_loss(nn)"]},{"cell_type":"markdown","metadata":{"id":"QNfbdGqrDkC7"},"source":["### 6.3.1 Recurrent Neural Networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zichd3O8mEI3"},"outputs":[],"source":["# dataset = train_batch\n","\n","# # create vocabulary\n","# vocab_size = 1000\n","# encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n","# encoder.adapt(dataset.map(lambda text, label: text))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mk3ma4E7bwe9","outputId":"4f3c0800-2426-4514-fcf0-e264539a4e1b"},"outputs":[],"source":["# # define the model\n","# model = tf.keras.Sequential([\n","#     # encoding layer\n","#     encoder,\n","#     # embedding layer\n","#     tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True),\n","#     # batch normalization layer (handles vanishing gradients)\n","#     tf.keras.layers.BatchNormalization(),\n","#     # rnn layer\n","#     tf.keras.layers.SimpleRNN(32),\n","#     # batch normalization layer (handles vanishing gradients)\n","#     tf.keras.layers.BatchNormalization(),\n","#     # output layer\n","#     tf.keras.layers.Dense(1, activation='sigmoid')\n","#     ])\n","\n","# # compile the model\n","# model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","#               optimizer=tf.keras.optimizers.Adam(),\n","#               metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","\n","# # fit the model to the training data\n","# rnn = model.fit(train_batch, validation_data=val_batch, epochs=10, callbacks=[early_stopping], class_weight=class_weights_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot_accuracy_loss(rnn)"]},{"cell_type":"markdown","metadata":{"id":"_G4LDPF6DkC7"},"source":["### 6.3.2 LSTM"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":11197,"status":"ok","timestamp":1685454485574,"user":{"displayName":"Lukas Stark","userId":"11679398301147369299"},"user_tz":-60},"id":"qcLSzawwfB_u"},"outputs":[],"source":["dataset = train_batch\n","\n","# create vocabulary\n","vocab_size = 1000\n","encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n","encoder.adapt(dataset.map(lambda text, label: text))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7LhupW4DkC7","outputId":"247a8912-e7f0-4c63-c7e5-70fc86ce184f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","16/80 [=====>........................] - ETA: 25:46:01 - loss: 0.6241 - accuracy: 0.8031 - precision: 0.6196 - recall: 0.8319"]}],"source":["# define the model\n","model = tf.keras.Sequential([\n","    # encoding layer\n","    encoder,\n","    # embedding layer\n","    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True),\n","    # lstm layer\n","    tf.keras.layers.LSTM(64),\n","    # output layer\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","\n","# compile the model\n","model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","\n","# fit the model to the training data\n","lstm = model.fit(train_batch, validation_data=val_batch, epochs=10, callbacks=[early_stopping], class_weight=class_weights_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mReJ_pzoHBMr"},"outputs":[],"source":["plot_accuracy_loss(lstm)"]},{"cell_type":"markdown","metadata":{"id":"UikFpj-XDkC8"},"source":["## 6.4 Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"DCaAfc9jJ9eN"},"source":["Bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ubYgcyKc7pz"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["Y-M1169-7Fkt","mk5tfM-27Fky","xqFhw_b3ohpN","6fSUIrfC7Fkz","Bt9h3ODfDkC4","sNa7WxNRn3QF"],"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
